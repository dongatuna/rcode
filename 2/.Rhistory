source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
summary(model.y1, model.y2)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
paretoPlot(model.y1)
library(pid)
data(solar)
model.y1<-lm(y1~A*B*C*D, data=solar)
summary(model.y1)
paretoPlot(model.y1)
model.y2<-lm(y2~A*B*C*D, data=solar)
summary(model.y2)
paretoPlot(model.y2)
#code the factors
H<-I<-S<-c(-1, +1)
S
design<-expand.grid(H=H, I=I, S=S)
H<-design$H
I<-design$I
S<-design$S
H
HIS
H, I, S
design
symptoms<-lm(y~H*I*S)
summary(symptoms)
source('~/.active-rstudio-document', echo=TRUE)
library(pid)
help(tradeOffTable)
library("pid", lib.loc="~/R/win-library/3.4")
library("pid", lib.loc="~/R/win-library/3.4")
source('C:/Users/ngatu/Dropbox/running_experiments/5 II/1.R', echo=TRUE)
#ungraded practice quiz number 1
#objective is to minimize weeds that sprout using soil solarization process
#the independent factors used are:
#factor P = duration of solarization period: low level is 4 weeks, high level is 6 weeks
#factor T = thickness of plastic: low level is 0.05 mm, high level is 0.1 mm
P<-c(-1, +1, -1, +1, 0)
#T<-c(-1, -1, +1, +1)
#the outcome variable, y, is the number of weeds that sprouted in soil patch after a week solarization
y<-c(12, 5, 13, 6, 8)
#the linear model of the outcome variable
number_of_weeds<-lm(y~P+I(P^2))
#the summary of the linear model
summary(number_of_weeds)
8-3.5*3+3^2
#ungraded practice quiz number 2
#the objective is to maximize followers and number of retweets
#the independent factors are:
#factor N = number of posts per day: low level is 2, high level is 5
#factor R = ratio of original content to re-post content: low level is 1:2, high level is 2:1
#factor T = tone of the message: low level is casual, high level is formal and authoritative
N<-c(-1, +1, -1, +1)
R<-c(-1, -1, +1, +1)
#factor T is generated from factors N and R
T<-N*R
#there are 2 outcome variables - the first one, y1, the number of new followers
y1<-c(125, 130, 120, 135)
#the second outcome variable, y2, is the number of re-tweets
y2<-c(75, 100, 78, 103)
#linear model for new followers, y1, and number of re-tweets, y2
new_followers<-lm(y1~N*R*T)
number_of_retweets<-lm(y2~N*R*T)
#the summary of y1, the number of new followers
summary(new_followers)
summary(number_of_retweets)
#ungraded practice quiz number 2
#the objective is to maximize followers and number of retweets
#the independent factors are:
#factor N = number of posts per day: low level is 2, high level is 5
#factor R = ratio of original content to re-post content: low level is 1:2, high level is 2:1
#factor T = tone of the message: low level is casual, high level is formal and authoritative
N<-c(-1, +1, -1, +1, 0)
R<-c(-1, -1, +1, +1, 0)
#factor T is generated from factors N and R
T<-c(+1, -1, -1, +1, +1)
#there are 2 outcome variables - the first one, y1, the number of new followers
y1<-c(125, 130, 120, 135, 128)
#the second outcome variable, y2, is the number of re-tweets
#y2<-c(75, 100, 78, 103)
#linear model for new followers, y1, and number of re-tweets, y2
new_followers<-lm(y1~N*R*T)
#number_of_retweets<-lm(y2~N*R*T)
#the summary of y1, the number of new followers
summary(new_followers)
H<-S<-c(-1, +1)
design<-expand.grid(H=H, S=S)
H<-design$H
S<-design$S
S
H
#question 3 in ungraded practice quiz 5A - 5D
#the outcome is to minimize the program costs of recyling electronics devices
#the independent factors and hours per week and number of recycling depot opened
#factor H = hours per week the sites are open: low level is 8 hours, high level is 12 hours
#factor S = number of electronics recyling depot sites available: low level is 2, high level is 6
H<-S<-c(-1, +1)
design<-expand.grid(H=H, S=S)
H<-design$H
S<-design$S
#the outcome variable, y, is the program cost per tonne of electronics recycled
y<-c(200, 150, 120, 100)
#the linear model
cost_per_tonne<-lm(y~H*S)
#summary of the linear model
summary(cost_per_tonne)
#question 4 in ungraded practice quiz 5A - 5D
#the objective is to maximize the number of students enrolled in the program
#the are 2 independent factors
#factor L = length of class: low level is 1.5 hours, high level is 3 hours
#factor N = number of the program runs for: low level is 4 weeks, high level is 6 weeks
L<-c(-1, 1, -1, 1)
N<-c(-1, -1, 1, 1)
#the outcome variable, y, is the number of enrolled students
y<-c(85, 70, 50, 40)
#the linear model fo outcome variable
students_enrolled<-lm(y~L*N)
#summary of the linear model
summary(students_enrolled)
61.25-6.25*-7/3-16.25*-2+1.25*14/3
77-2*2-1.5*4
67-4*2-5+.7*2
#question 6 of ungraded practice quiz 5A - 5D
#the outcome variable is range of temperature predetermined by a cooling setting
#the objective is to minimize that temperature
#independent factors to investigate
#factor A is compressor power: low level is 300W, high level is 500W
#factor B is refrigerator size: low level is 50 L and high level is 80 L
#factor C is insulation thickness: low level is 8 cm, high level is 12 cm
B<-c(-1, -1, +1, +1)
A<-c(-1, +1, -1, +1)
#factor C is generated from factors A and B
C<-A*B
#the outcome, y, the final steady temperature
y<-c(8, 3, 8, 4)
#the linear model
temp_dir<-lm(y~A*B*C)
#summary of the linear model
summary(temp_dir)
#question 6 of ungraded practice quiz 5A - 5D
#the outcome variable is range of temperature predetermined by a cooling setting
#the objective is to minimize that temperature
#independent factors to investigate
#factor A is compressor power: low level is 300W, high level is 500W
#factor B is refrigerator size: low level is 50 L and high level is 80 L
#factor C is insulation thickness: low level is 8 cm, high level is 12 cm
#B<-c(-1, -1, +1, +1)
A<-c(-1, +1, -1, +1, 3)
#factor C is generated from factors A and B
#C<-A*B
#the outcome, y, the final steady temperature
y<-c(8, 3, 8, 4, -6)
#the linear model
temp_dir<-lm(y~A+I(A^2))
#summary of the linear model
summary(temp_dir)
6.38-2.25*2-.625*4
6.38-2.25*2-.625*4
10^0.95
10^0.97
100^0.95
100^0.97
x <- as.integer(runif(50, 2, 11))
library("pid", lib.loc="~/R/win-library/3.4")
x <- as.integer(runif(50, 2, 11))
x <- as.integer(runif(50, 2, 11))
x <- as.integer(runif(50, 2, 11))
x <- as.integer(runif(50, 2, 11))
sd(x)
sd(x)
var(x)
MAD(x)
median(x)
x <- as.integer(runif(50, 2, 11))
sd(x)
var(x)
median(x)
MAD(x)
mad(x)
pnorm(-1, mean=0, sd=1)
v.yield = c(23, 19, 17, 18, 24, 26, 21, 14, 18)
summary(v.yield)
v.sd<-sd(v.yield)
v.sd
v.sd<-sd(v.yield)
sd(v.yield)
index<-(N-0.05)/N
N = 9;
index<-seq(1, N)
z.ideal = (index -0.05)/N
N = 9;
index<-seq(1, N)
z.ideal = (index -0.05)/N
z.ideal
N = 9;
index<-seq(1, N)
# there are 9 observations, so N = 9
N = 9;
index<-seq(1, N)
index
N = 9;
index<-seq(1, N)
z.ideal = (index -0.05)/N
theoretical.z<-qnorm(z.ideal)
library("car", lib.loc="~/R/win-library/3.4")
theoretical.z
hist(theoretical.z)
v.yield = c(23, 19, 17, 18, 24, 26, 21, 14, 18)
z.mean <- mean(v.yield)
z.sd <- sd(v.yield)
z.actual <- sort(v.yield - z.mean)/z.sd
actual.z<-qnorm(z.actual)
hist(actual.z)
plot(theoretical.z, actual.z, p)
N = 9;
index<-seq(1, N)
z.ideal = (index -0.05)/N
theoretical.z<-qnorm(z.ideal)
theoretical.z
#hist(theoretical.z)
v.yield = c(23, 19, 17, 18, 24, 26, 21, 14, 18)
z.mean <- mean(v.yield)
z.sd <- sd(v.yield)
z.actual <- sort(v.yield - z.mean)/z.sd
actual.z<-qnorm(z.actual)
plot(theoretical.z, actual.z, type = "p")
N = 9;
index<-seq(1, N)
z.ideal = (index -0.05)/N
theoretical.z<-qnorm(z.ideal)
theoretical.z
#hist(theoretical.z)
v.yield = c(23, 19, 17, 18, 24, 26, 21, 14, 18)
z.mean <- mean(v.yield)
z.sd <- sd(v.yield)
z.actual <- sort(v.yield - z.mean)/z.sd
actual.z<-qnorm(z.actual)
plot(theoretical.z, actual.z, type = "p")
library(car)
qqPlot(v.yield)
qqPlot(z.actual)
qt(8)
qt(df=8)
qt(0.975, df=8) #gives the z ordinate
z.mean - Ct*z.sd/3
Ct <- qt(0.975, df=8) #gives the z ordinate, given a 0.975 area under the curve and a df of 8
z.mean - Ct*z.sd/3
z.mean + Ct*z.sd/3
pt(z.value, df = 58)
z.value<-{mean(saturday$Visits) - mean(friday$Visits)}/(pooled.var/15)
z.value
# mean.saturday - mean.friday = -5.5 < 0 and therefore z.value is negative.
# use qt(z.value, df=58) to find what the probability distribution the difference between the mean parameters is zero.
# the qt value is 4 %
# There is less than 1 in 20 chances that the difference in p
pt(z.value, df = 58)
z.value<-{mean(saturday$Visits) - mean(friday$Visits)}/(pooled.var/15)
# 1
# Calculate the mean and the variance of a 6 sided die
dice<-c(1, 2, 3, 4, 5, 6)
mean(dice)
sd(dice)
var(dice)
#------------------------------------------------------------------------
# 2
# Compute mean, median, sd and MAD
salt<-c(460, 520, 580, 700, 760, 770, 890, 910, 920, 940, 960, 1060, 1100)
summary(salt) #mean = 813.1, median = 890
#the robust measure of spread, MAD
mad(salt)
# calculate the IQR
# 1st quartile = 700, 3rd quartile = 940
summary(salt)
IQR(salt)
boxplot(salt, ylab="mg/salt per 15 mL serving", main="Salt Concentration" )
#box plot is not symmetric, meaning that the distribution is not likely to be normal
library(car)
qqPlot(salt)
#-------------------------------------------------------------------------
#4
rm<-read.csv('raw-material-properties.csv')
head(rm)
ncol(rm) #counts the unique number of columns
nrow(rm) #counts the unique number of rows
# Plot the data for size 1
plot(rm$size1, ylab="Particle size: level 1")
boxplot(rm$size1)
# This function identifies or tags point points on a scatter plot
identify(rm$size1, labels=rm$Sample)
plot(rm$size2, ylab="Particle size: level 2")
boxplot(rm$size2)
identify(rm$size2, labels = rm$Sample)
plot(rm$size3, ylab = "Particle size: level 3")
identify(rm$size3, labels = rm$Sample)
plot(rm$density1, ylab="Particle density: level 1")
identify(rm$density1, labels=rm$Sample)
plot(rm$density2, ylab="Particle density: level 2")
identify(rm$density2, labels=rm$Sample)
plot(rm$density3, ylab="Particle density: level 3")
identify(rm$density3, labels=rm$Sample)
# The lessons here is that outliers are must be visually spotted
# Thus the need to plot the data.  No reliable software exists to detect outliers
#------------------------------------------------------------------
#5
# Feedback control is an artificial created variability designed to reduce process variability.
#
# The costs of variability to producers include: rework/inspection costs, disposing products, and discount pricing
# The costs of variability to users include: additional work, no work, low product
#
#-------------------------------------------------------------------
# 7
# import the data
webdata<-read.csv('website-traffic.csv')
# show the head rows
head(webdata)
summary(webdata$Visits)
# plot the histogram
hist(webdata$Visits, xlab = "Number of visits", main = "Website Visits")
# check whether webdata come from normal distribution
# import car library
library(car)
qqPlot(webdata$Visits, ylab = "Website Visits")
web.mean <- mean(webdata$Visits)# The mean is 22.23
web.count <- length(webdata$Visits) # 214
web.sd <- sd(webdata$Visits)
z.10.score<-(10-web.mean)/web.sd # -1.4683
pnorm(z.10.score) # 0.071
z.30.score<-(30-web.mean)/web.sd # 0.9321
pnorm(z.30.score) # 0.824
# The likelihood that 0.824 - 0.071
0.824 - 0.071
#---------------------------------------------------------------------------
# 8
ammonia <-read.csv('ammonia.csv')
head(ammonia)
length(ammonia$Ammonia)
library(car)
qqPlot(ammonia$Ammonia, ylab = 'Ammonia')
fitdistr(ammonia$Ammonia, 'normal')
#---------------------------------------------------------------------------
# 9
viscosity<-c(23, 19, 17, 18, 24, 26, 21, 14, 18)
N=9
mean(viscosity) # The mean is 20
sd(viscosity) # The sd is 3.81
# The distribution of the sample average is normal distribution
# The parameters of the distribution are never known
qnorm(0.025) #1.96
qt(0.025, df = 8) # -2.31
lowbound<-(20-2.31*3.81/sqrt(8)) # 16.89
upperbound<-(20+2.31*3.81/sqrt(8)) # 23.11
lowbound<-(20-1.96*3.5/sqrt(8)) # 17.14
upperbound<-(20+1.96*3.5/sqrt(8)) # 22.86
#-----------------------------------------------------------------------------
# 10
# sd = 40cP
# UB - LB = 60cP
(1.96*40/30)^2
#-------------------------------------------------------------------
AvgLB<-14.67-1.16*-qt(0.025, df=11)/sqrt(12)
AvgLB
AvgUB<-14.67+1.16*qt(0.975, df=11)/sqrt(12)
AvgUB
LB<-14.67-1.16*qt(0.025, df=11)
LB
UB<-14.67+1.16*qt(0.975, df=11)
UB
#---------------------------------------------------------------------
# 13
yield<-read.csv('batch-yields.csv')
head(yield)
library(car)
qqPlot(yield$Yield)
hist(yield$Yield, main = 'Yields from a batch bioreactor')
#-----------------------------------------------------------------------
#16
bv<-read.csv('brittleness-index.csv')
head(bv)
summary(bv$TK104)
summary(bv$TK105)
summary(bv$TK107)
var(bv$TK104)
var(bv$TK105)
var(bv$TK107)
# ----------------------------------------------------------------------
# 17
# import the file
traffic<-read.csv('website-traffic.csv')
# read the table head columns
head(traffic)
# fetch friday's data and summarize
friday<-subset(traffic, traffic$DayOfWeek=='Friday')
sd(friday$Visits)
summary(friday$Visits)
# fetch saturday's data and summarize
saturday<-subset(traffic, traffic$DayOfWeek=='Saturday')
sd(saturday$Visits)
summary(saturday$Visits)
# get the count of fridays and saturdays
length(friday$Visits) # 30
length(saturday$Visits) # 30
# to test the difference in average visits on a Friday and a Saturday
# calculate z value for the differences of averages
# z = (avg.sat.visits - avg.fri.visits)-(avg.par.sat.visits - avg.par.fri.visits)/pooled sd
# confirm that you can pool the variances
qf(0.025, 29, 29) # 0.48
qf(0.975, 29, 29) # 2.1
pooled.var<-((29)*var(saturday$Visits)+(29)*var(friday$Visits))/58
pooled.var #47.1
# since the pooled variance uses t-distribution, confirm both plots come from a normal dist
qqPlot(friday$Visits) # normally distributed
qqPlot(saturday$Visits) # normally distributed
# test the differences between Friday and Saturday's averages
# when using pooled variance, df = sample size a + sample size b - 2
# df = count of 'Fridays' + count of 'Saturdays' - 2
df = 58 # (30-1)+(30-1)
# APPROACH TO CALCULATE Zdiff
# Assume the differences in the parameter averages is zero
# calculate z value = [(avg.sampleofA - avg.sampleofB)-0]/sqrt(pooledvar*2/n)
# pnorm(z value) or pt(z value)
z.value<-{mean(saturday$Visits) - mean(friday$Visits)}/(pooled.var/15)
z.value
# mean.saturday - mean.friday = -5.5 < 0 and therefore z.value is negative.
# use qt(z.value, df=58) to find what the probability distribution the difference between the mean parameters is zero.
# the qt value is 4 %
# There is less than 1 in 20 chances that the difference in p
pt(z.value, df = 58)
# 3.697
pt(1.751956, df=58)
pooled.var<-((29)*var(saturday$Visits)+(29)*var(friday$Visits))/58
pooled.var
# 17
# import the file
traffic<-read.csv('website-traffic.csv')
# read the table head columns
head(traffic)
traffic<-read.csv('website-traffic.csv')
# import the file
traffic<-read.csv('website-traffic.csv')
setwd("C:/Users/ngatu/Desktop/rcode/2")
# import the file
traffic<-read.csv('website-traffic.csv')
# read the table head columns
head(traffic)
# fetch friday's data and summarize
friday<-subset(traffic, traffic$DayOfWeek=='Friday')
sd(friday$Visits)
summary(friday$Visits)
# fetch saturday's data and summarize
saturday<-subset(traffic, traffic$DayOfWeek=='Saturday')
sd(saturday$Visits)
summary(saturday$Visits)
# get the count of fridays and saturdays
length(friday$Visits) # 30
length(saturday$Visits) # 30
# to test the difference in average visits on a Friday and a Saturday
# calculate z value for the differences of averages
# z = (avg.sat.visits - avg.fri.visits)-(avg.par.sat.visits - avg.par.fri.visits)/pooled sd
# confirm that you can pool the variances
qf(0.025, 29, 29) # 0.48
qf(0.975, 29, 29) # 2.1
pooled.var<-((29)*var(saturday$Visits)+(29)*var(friday$Visits))/58
pooled.var #47.1
# since the pooled variance uses t-distribution, confirm both plots come from a normal dist
qqPlot(friday$Visits) # normally distributed
qqPlot(saturday$Visits) # normally distributed
# test the differences between Friday and Saturday's averages
# when using pooled variance, df = sample size a + sample size b - 2
# df = count of 'Fridays' + count of 'Saturdays' - 2
df = 58 # (30-1)+(30-1)
# APPROACH TO CALCULATE Zdiff
# Assume the differences in the parameter averages is zero
# calculate z value = [(avg.sampleofA - avg.sampleofB)-0]/sqrt(pooledvar*2/n)
# pnorm(z value) or pt(z value)
z.value<-{mean(saturday$Visits) - mean(friday$Visits)}/(pooled.var/15)
z.value
# mean.saturday - mean.friday = -5.5 < 0 and therefore z.value is negative.
# use qt(z.value, df=58) to find what the probability distribution the difference between the mean parameters is zero.
# the qt value is 4 %
# There is less than 1 in 20 chances
pt(z.value, df = 58)
pt(1.751956, df=58)
y<-c(11, 26, 18, 16, 20, 12, 8, 26, 12, 17, 14)
x<-c(25, 3, 27, 30, 33, 16, 28, 27, 12, 32, 16)
summary(x)
summary(y)
library(car)
qqPlot(y, ylab='Dilution method')
library(car)
qqPlot(y, main = 'Dilution method')
qqPlot(x, main = 'Manometric method')
var(y)
var(x)
qf(0.025, 10, 10)
qf(0.975, 10, 10)
var.xy<-10(var(x)+var(y))/20
var.xy<-10*(var(x)+var(y))/20
qt(z, df=20)
z<-(mean(x)-mean(y))/sqrt(var.xy*1/11)
#note the df = (11-1) + (11-1)
qt(z, df=20)
z<-(mean(x)-mean(y))/sqrt(var.xy*1/11)
z
pt(z, df=20)
qnorm(0.01)
sd.chips<-abs((37.5-35)/qnorm(0.01))
sd.chips
z.chips<-(40-37.4)/sd.chips
z.chips<-(40-37.4)/sd.chips
pnorm(z.chips)
